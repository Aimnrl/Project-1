"""
Scrape search results from YouTube using Selenium headless Chrome.
Saves data to data/raw_scraped.parquet

WARNING: Scraping public sites may be against TOS. Use responsibly.
"""
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urlparse, parse_qs
import pandas as pd
import time
import os
from tqdm import tqdm

OPTIONS = Options()
OPTIONS.add_argument('--headless=new')
OPTIONS.add_argument('--no-sandbox')
OPTIONS.add_argument('--disable-dev-shm-usage')
OPTIONS.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")


def parse_video_id(href):
    if not href:
        return None
    try:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        return qs.get('v', [None])[0]
    except Exception:
        return None


def scrape_query(driver, query, scrolls=3):
    url = f'https://www.youtube.com/results?search_query={query.replace(" ", "+")}'
    driver.get(url)
    time.sleep(2) # Wait for initial load
    
    # Scroll to load more results
    for _ in range(scrolls):
        driver.execute_script('window.scrollTo(0, document.documentElement.scrollHeight);')
        time.sleep(1.2)
        
    cards = driver.find_elements(By.CSS_SELECTOR, 'ytd-video-renderer, ytd-grid-video-renderer')
    data = []
    for c in cards:
        try:
            a = c.find_element(By.CSS_SELECTOR, 'a#thumbnail')
            href = a.get_attribute('href')
            vid = parse_video_id(href)
            if not vid:
                continue
                
            title = c.find_element(By.CSS_SELECTOR, '#video-title').text
            channel = c.find_element(By.CSS_SELECTOR, 'ytd-channel-name a').text
            meta = c.find_element(By.CSS_SELECTOR, '#metadata-line').text
            data.append({'video_id':vid, 'title': title, 'channel_title': channel, 'meta': meta})
        except Exception:
            continue
    return data


if __name__ == '__main__':
    try:
        driver = webdriver.Chrome(options=OPTIONS)
    except Exception as e:
        print(f"Error initializing Selenium driver: {e}")
        print("Please ensure chromedriver is installed and in your PATH.")
        exit()

    base_queries = ['music','news','gaming','tutorial','cooking','comedy','education','tech']
    # We don't need to multiply by 400, just iterate until we have enough data
    
    all_rows = {} # Use dict for automatic de-duplication
    target_videos = 3200
    
    with tqdm(total=target_videos, desc="Scraping videos") as pbar:
        for q in base_queries * 50: # Iterate over the list multiple times if needed
            if len(all_rows) >= target_videos:
                break
            try:
                rows = scrape_query(driver, q, scrolls=2)
                
                new_vids = 0
                for r in rows:
                    if r['video_id'] and r['video_id'] not in all_rows:
                        all_rows[r['video_id']] = r
                        new_vids += 1
                
                pbar.update(new_vids)
                
            except Exception as e:
                print(f'Error scraping query "{q}": {e}')
                time.sleep(1)

    df = pd.DataFrame(all_rows.values())
    if len(df) > target_videos:
        df = df.iloc[:target_videos]

    os.makedirs('data', exist_ok=True)
    df.to_parquet('data/raw_scraped.parquet', index=False)
    driver.quit()
    print(f'\nScraped {len(df)} unique videos.')
